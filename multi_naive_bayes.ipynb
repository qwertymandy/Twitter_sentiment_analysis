{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>atswitchfoot http://twitpic.com/2y1zl - Awww, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>atKenichan I dived many times for the ball. Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>atnationwideclass no, it's not behaving at all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday attheNSPCC atSparksChari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "0        _TheSpecialOne_  atswitchfoot http://twitpic.com/2y1zl - Awww, ...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  atKenichan I dived many times for the ball. Ma...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  atnationwideclass no, it's not behaving at all...  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday attheNSPCC atSparksChari...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "tweets=pd.read_csv('training.1600000.processed.noemoticon.csv',encoding='latin', \n",
    "                   names = ['sentiment','id','date','query','user','tweet'])\n",
    "tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200000, 6)\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.sample(frac=1)\n",
    "tweets = tweets[:200000]\n",
    "print(\"Dataset shape:\", tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169171</th>\n",
       "      <td>1</td>\n",
       "      <td>1980232058</td>\n",
       "      <td>Sun May 31 06:07:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mick_boffa</td>\n",
       "      <td>loving summer .. summer loving ... i love summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084904</th>\n",
       "      <td>1</td>\n",
       "      <td>1969075775</td>\n",
       "      <td>Fri May 29 22:58:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>algonacchick</td>\n",
       "      <td>atludovicah - I'm just surprised he's never ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558990</th>\n",
       "      <td>1</td>\n",
       "      <td>2185964791</td>\n",
       "      <td>Mon Jun 15 18:09:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>juuhmtv</td>\n",
       "      <td>atjuniorsk8tista jÃ¡te add no msn amor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068748</th>\n",
       "      <td>1</td>\n",
       "      <td>1965789649</td>\n",
       "      <td>Fri May 29 16:32:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>GratiaeUtDeus</td>\n",
       "      <td>atmineralrich atMomOfMercy atcatholicseeking T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401115</th>\n",
       "      <td>0</td>\n",
       "      <td>2057585198</td>\n",
       "      <td>Sat Jun 06 13:16:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Leanne0710</td>\n",
       "      <td>atnicole_b86 a dnt like them haha  #hateperez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>0</td>\n",
       "      <td>1469837334</td>\n",
       "      <td>Tue Apr 07 07:33:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Emogirltalk</td>\n",
       "      <td>On my way to school. I have been wearing my re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238865</th>\n",
       "      <td>1</td>\n",
       "      <td>1993393712</td>\n",
       "      <td>Mon Jun 01 10:18:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Twaysive</td>\n",
       "      <td>atcherylp3 heh Todd must have told you my ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46596</th>\n",
       "      <td>0</td>\n",
       "      <td>1677444250</td>\n",
       "      <td>Sat May 02 02:06:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChristineyBeany</td>\n",
       "      <td>atgillianvirginia Well, actually, I only have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232662</th>\n",
       "      <td>0</td>\n",
       "      <td>1979213393</td>\n",
       "      <td>Sun May 31 02:05:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lothrek</td>\n",
       "      <td>Awoken early. Have a sore neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716674</th>\n",
       "      <td>0</td>\n",
       "      <td>2259768872</td>\n",
       "      <td>Sat Jun 20 18:07:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CoachJohnWright</td>\n",
       "      <td>atholdenbeach my brother and his wife are down...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "1169171          1  1980232058  Sun May 31 06:07:25 PDT 2009  NO_QUERY   \n",
       "1084904          1  1969075775  Fri May 29 22:58:11 PDT 2009  NO_QUERY   \n",
       "1558990          1  2185964791  Mon Jun 15 18:09:44 PDT 2009  NO_QUERY   \n",
       "1068748          1  1965789649  Fri May 29 16:32:48 PDT 2009  NO_QUERY   \n",
       "401115           0  2057585198  Sat Jun 06 13:16:40 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "7478             0  1469837334  Tue Apr 07 07:33:54 PDT 2009  NO_QUERY   \n",
       "1238865          1  1993393712  Mon Jun 01 10:18:18 PDT 2009  NO_QUERY   \n",
       "46596            0  1677444250  Sat May 02 02:06:26 PDT 2009  NO_QUERY   \n",
       "232662           0  1979213393  Sun May 31 02:05:29 PDT 2009  NO_QUERY   \n",
       "716674           0  2259768872  Sat Jun 20 18:07:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1169171       mick_boffa  loving summer .. summer loving ... i love summ...  \n",
       "1084904     algonacchick  atludovicah - I'm just surprised he's never ev...  \n",
       "1558990          juuhmtv            atjuniorsk8tista jÃ¡te add no msn amor   \n",
       "1068748    GratiaeUtDeus  atmineralrich atMomOfMercy atcatholicseeking T...  \n",
       "401115        Leanne0710      atnicole_b86 a dnt like them haha  #hateperez  \n",
       "...                  ...                                                ...  \n",
       "7478         Emogirltalk  On my way to school. I have been wearing my re...  \n",
       "1238865         Twaysive  atcherylp3 heh Todd must have told you my ques...  \n",
       "46596    ChristineyBeany  atgillianvirginia Well, actually, I only have ...  \n",
       "232662           lothrek                    Awoken early. Have a sore neck   \n",
       "716674   CoachJohnWright  atholdenbeach my brother and his wife are down...  \n",
       "\n",
       "[200000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiment']=tweets['sentiment'].replace(4,1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169171</th>\n",
       "      <td>1</td>\n",
       "      <td>loving summer .. summer loving ... i love summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084904</th>\n",
       "      <td>1</td>\n",
       "      <td>atludovicah - I'm just surprised he's never ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558990</th>\n",
       "      <td>1</td>\n",
       "      <td>atjuniorsk8tista jÃ¡te add no msn amor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068748</th>\n",
       "      <td>1</td>\n",
       "      <td>atmineralrich atMomOfMercy atcatholicseeking T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401115</th>\n",
       "      <td>0</td>\n",
       "      <td>atnicole_b86 a dnt like them haha  #hateperez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546556</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey #marklowry  Hello from Oshkosh, WI    (Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41107</th>\n",
       "      <td>0</td>\n",
       "      <td>i really dont wanna go inside and work - waste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700583</th>\n",
       "      <td>0</td>\n",
       "      <td>Going to look at nissan cubes then target. I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810358</th>\n",
       "      <td>1</td>\n",
       "      <td>Am too tired, with classes right from 9 in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588063</th>\n",
       "      <td>1</td>\n",
       "      <td>atmissy_el Lunch once this essay is done is a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet\n",
       "1169171          1  loving summer .. summer loving ... i love summ...\n",
       "1084904          1  atludovicah - I'm just surprised he's never ev...\n",
       "1558990          1            atjuniorsk8tista jÃ¡te add no msn amor \n",
       "1068748          1  atmineralrich atMomOfMercy atcatholicseeking T...\n",
       "401115           0      atnicole_b86 a dnt like them haha  #hateperez\n",
       "1546556          1  Hey #marklowry  Hello from Oshkosh, WI    (Mar...\n",
       "41107            0  i really dont wanna go inside and work - waste...\n",
       "700583           0  Going to look at nissan cubes then target. I l...\n",
       "810358           1  Am too tired, with classes right from 9 in the...\n",
       "1588063          1  atmissy_el Lunch once this essay is done is a ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.drop(['date','query','user'], axis=1, inplace=True)\n",
    "tweets.drop('id', axis=1, inplace=True)\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0.0\n",
       "tweet        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tweets.isnull().sum() / len(tweets))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting pandas object to a string type\n",
    "tweets['tweet'] = tweets['tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         200000\n",
      "No. of positve tagged sentences is:  100097\n",
      "No. of negative tagged sentences is: 99903\n"
     ]
    }
   ],
   "source": [
    "positives = tweets['sentiment'][tweets.sentiment == 1 ]\n",
    "negatives = tweets['sentiment'][tweets.sentiment == 0 ]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(tweets.shape[0]))\n",
    "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you', 'were', 'mightn', 'mustn', 'was', 'yourself', 'not', 'did', 'herself', 'such', \"won't\", 'then', 'o', 'in', 'hers', 'this', \"shouldn't\", 'there', 'we', 'needn', 'off', 'y', \"hasn't\", \"isn't\", 'doing', 't', 'same', 'on', 'after', 'each', 'his', 'these', 'whom', \"mightn't\", 'ourselves', 'i', 'once', 'will', 'just', 'down', 'having', \"don't\", 'for', 'wouldn', 'of', 'only', 'he', \"didn't\", 'against', 'why', 'am', 'about', 'our', 'those', 'so', 'more', 'she', 'both', 'other', 'during', 'below', 'be', 'my', 'theirs', 'them', \"should've\", 'isn', \"it's\", \"she's\", 'does', 'than', 'aren', 'itself', 'own', 'before', 'into', 'further', 'from', 'which', \"you're\", 'wasn', 'the', 'doesn', 'it', 'few', 'some', 'but', 'above', 'won', 'hasn', 'its', 'been', 'too', 'can', \"you've\", \"doesn't\", 'nor', 'under', 'or', 'where', 'hadn', 'while', 'me', \"aren't\", \"weren't\", 'shan', 'over', 'if', 'had', 'out', 'should', 'shouldn', 'by', 'with', \"wasn't\", 'have', \"mustn't\", 'at', 'didn', 'any', 'no', 'what', 'm', 'themselves', 'your', 'ours', 'yours', 've', \"you'll\", \"haven't\", 'who', 'all', 'do', 'don', 'd', 's', 'because', \"wouldn't\", 'him', 'haven', 'through', \"you'd\", 'ma', \"shan't\", \"needn't\", 'weren', 'an', 'yourselves', 'himself', 'when', 'ain', 'very', 'has', 'are', 'being', 'here', 'until', 'between', 'myself', 're', 'now', 'and', 'll', 'their', 'a', 'how', 'most', 'they', 'to', \"couldn't\", \"that'll\", 'that', 'couldn', \"hadn't\", 'up', 'her', 'as', 'again', 'is'}\n"
     ]
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = r\"[@^+\\s]\"\n",
    "some = 'amp,today,tomorrow,going,girl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet) \n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
    "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
    "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
    "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
    "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
    "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
    "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
    "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
    "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
    "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
    "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
    "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
    "    tweet = re.sub(r\"goood\", \"good\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet=tweet[1:]\n",
    "    # Removing all URls \n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet) \n",
    "    #remove some words\n",
    "    tweet= re.sub(some,'',tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #tokens = [w for w in tokens if len(w)>2]\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem \n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "     \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "     \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "     \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev_in_text(tweet):\n",
    "    t=[]\n",
    "    words=tweet.split()\n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
    "    return ' '.join(t)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169171</th>\n",
       "      <td>1</td>\n",
       "      <td>loving summer .. summer loving ... i love summ...</td>\n",
       "      <td>ovingsummersummerlovingilovesummerrr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084904</th>\n",
       "      <td>1</td>\n",
       "      <td>atludovicah - I'm just surprised he's never ev...</td>\n",
       "      <td>tludovicahiamjustsurprisedheisneverevencalledm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558990</th>\n",
       "      <td>1</td>\n",
       "      <td>atjuniorsk8tista jÃ¡te add no msn amor</td>\n",
       "      <td>tjuniorsk8tistajã¡teaddnomsnamor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068748</th>\n",
       "      <td>1</td>\n",
       "      <td>atmineralrich atMomOfMercy atcatholicseeking T...</td>\n",
       "      <td>tmineralrichatmomofmercyatcatholicseekingthank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401115</th>\n",
       "      <td>0</td>\n",
       "      <td>atnicole_b86 a dnt like them haha  #hateperez</td>\n",
       "      <td>tnicoleb86adntlikethemhahahateperez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>0</td>\n",
       "      <td>On my way to school. I have been wearing my re...</td>\n",
       "      <td>nmywaytoschoolihavebeenwearingmyretainerlately...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238865</th>\n",
       "      <td>1</td>\n",
       "      <td>atcherylp3 heh Todd must have told you my ques...</td>\n",
       "      <td>tcherylp3hehtoddmusthavetoldyoumyquestionlolju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46596</th>\n",
       "      <td>0</td>\n",
       "      <td>atgillianvirginia Well, actually, I only have ...</td>\n",
       "      <td>tgillianvirginiawellactuallyionlyhaveapairofsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232662</th>\n",
       "      <td>0</td>\n",
       "      <td>Awoken early. Have a sore neck</td>\n",
       "      <td>wokenearlyhaveasoreneck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716674</th>\n",
       "      <td>0</td>\n",
       "      <td>atholdenbeach my brother and his wife are down...</td>\n",
       "      <td>tholdenbeachmybrotherandhiswifearedownatholden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet  \\\n",
       "1169171          1  loving summer .. summer loving ... i love summ...   \n",
       "1084904          1  atludovicah - I'm just surprised he's never ev...   \n",
       "1558990          1            atjuniorsk8tista jÃ¡te add no msn amor    \n",
       "1068748          1  atmineralrich atMomOfMercy atcatholicseeking T...   \n",
       "401115           0      atnicole_b86 a dnt like them haha  #hateperez   \n",
       "...            ...                                                ...   \n",
       "7478             0  On my way to school. I have been wearing my re...   \n",
       "1238865          1  atcherylp3 heh Todd must have told you my ques...   \n",
       "46596            0  atgillianvirginia Well, actually, I only have ...   \n",
       "232662           0                    Awoken early. Have a sore neck    \n",
       "716674           0  atholdenbeach my brother and his wife are down...   \n",
       "\n",
       "                                          processed_tweets  \n",
       "1169171               ovingsummersummerlovingilovesummerrr  \n",
       "1084904  tludovicahiamjustsurprisedheisneverevencalledm...  \n",
       "1558990                   tjuniorsk8tistajã¡teaddnomsnamor  \n",
       "1068748  tmineralrichatmomofmercyatcatholicseekingthank...  \n",
       "401115                 tnicoleb86adntlikethemhahahateperez  \n",
       "...                                                    ...  \n",
       "7478     nmywaytoschoolihavebeenwearingmyretainerlately...  \n",
       "1238865  tcherylp3hehtoddmusthavetoldyoumyquestionlolju...  \n",
       "46596    tgillianvirginiawellactuallyionlyhaveapairofsu...  \n",
       "232662                             wokenearlyhaveasoreneck  \n",
       "716674   tholdenbeachmybrotherandhiswifearedownatholden...  \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['processed_tweets'] = tweets['tweet'].apply(lambda x: process_tweets(x))\n",
    "tweets['processed_tweets'] = tweets['processed_tweets'].apply(lambda x: convert_abbrev_in_text(x))\n",
    "print('Text Preprocessing complete.')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169171</th>\n",
       "      <td>1</td>\n",
       "      <td>loving summer .. summer loving ... i love summ...</td>\n",
       "      <td>ovingsummersummerlovingilovesummerrr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084904</th>\n",
       "      <td>1</td>\n",
       "      <td>atludovicah - I'm just surprised he's never ev...</td>\n",
       "      <td>tludovicahiamjustsurprisedheisneverevencalledm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558990</th>\n",
       "      <td>1</td>\n",
       "      <td>atjuniorsk8tista jÃ¡te add no msn amor</td>\n",
       "      <td>tjuniorsk8tistajã¡teaddnomsnamor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068748</th>\n",
       "      <td>1</td>\n",
       "      <td>atmineralrich atMomOfMercy atcatholicseeking T...</td>\n",
       "      <td>tmineralrichatmomofmercyatcatholicseekingthank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401115</th>\n",
       "      <td>0</td>\n",
       "      <td>atnicole_b86 a dnt like them haha  #hateperez</td>\n",
       "      <td>tnicoleb86adntlikethemhahahateperez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet  \\\n",
       "1169171          1  loving summer .. summer loving ... i love summ...   \n",
       "1084904          1  atludovicah - I'm just surprised he's never ev...   \n",
       "1558990          1            atjuniorsk8tista jÃ¡te add no msn amor    \n",
       "1068748          1  atmineralrich atMomOfMercy atcatholicseeking T...   \n",
       "401115           0      atnicole_b86 a dnt like them haha  #hateperez   \n",
       "\n",
       "                                          processed_tweets  \n",
       "1169171               ovingsummersummerlovingilovesummerrr  \n",
       "1084904  tludovicahiamjustsurprisedheisneverevencalledm...  \n",
       "1558990                   tjuniorsk8tistajã¡teaddnomsnamor  \n",
       "1068748  tmineralrichatmomofmercyatcatholicseekingthank...  \n",
       "401115                 tnicoleb86adntlikethemhahahateperez  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing shortwords\n",
    "tweets['processed_tweets']=tweets['processed_tweets'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "tweets =  shuffle(tweets).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [oookilosttodcinshootoutshowdown]\n",
       "1      [tbbysayhuhdirectmessageisentyouthelinkbuthere]\n",
       "2    [tdavidlebovitzthedayhereiswaningasunnyautumnd...\n",
       "3                             [tsueitsmeyoudonotbesad]\n",
       "4                         [tgabexmoshquotwithdonequot]\n",
       "Name: processed_tweets, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet=tweets['processed_tweets'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweets['processed_tweets'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (1.26.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = tweets[tweets['sentiment'] == 0]\n",
    "class_1_data = tweets[tweets['sentiment'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_prob_class_0 = len(class_0_data) / len(tweets)\n",
    "prior_prob_class_1 = len(class_1_data) / len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(word for tweet in tweets['processed_tweets'] for word in tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def calculate_prior(y):\n",
    "    \"\"\"\n",
    "    Calculate the prior probability P(y) for each class.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: numpy array, target variable\n",
    "    \n",
    "    Returns:\n",
    "    - priors: dictionary, prior probabilities for each class\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    priors = dict(zip(classes, counts / len(y)))\n",
    "    return priors\n",
    "\n",
    "def calculate_likelihood(X, y, vectorizer, alpha=0.001):\n",
    "    \"\"\"\n",
    "    Calculate the likelihood probabilities for each word and class.\n",
    "\n",
    "    Parameters:\n",
    "    - X: scipy sparse matrix, feature matrix\n",
    "    - y: numpy array, target variable\n",
    "    - vectorizer: CountVectorizer or TfidfVectorizer\n",
    "    - alpha: Laplace smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "    - likelihoods: 2D NumPy array, likelihood probabilities for each word and class\n",
    "    \"\"\"\n",
    "    if issparse(X):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    num_classes = len(np.unique(y))\n",
    "    num_features = X.shape[1]\n",
    "    likelihoods = np.zeros((num_classes, num_features))\n",
    "\n",
    "    for c in np.unique(y):\n",
    "        class_indices = (y == c)\n",
    "        class_word_counts = X[class_indices].sum(axis=0)\n",
    "        total_word_counts = class_word_counts.sum()\n",
    "\n",
    "        # Laplace smoothing\n",
    "        likelihoods[c, :] = (class_word_counts + alpha) / (total_word_counts + alpha * num_features)\n",
    "\n",
    "    return likelihoods\n",
    "\n",
    "def naive_bayes_multinomial(X, priors, likelihoods, alpha=0.001):\n",
    "    \"\"\"\n",
    "    Make predictions using the Multinomial Naive Bayes model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: scipy sparse matrix, feature matrix\n",
    "    - priors: dictionary, prior probabilities for each class\n",
    "    - likelihoods: 2D NumPy array, likelihood probabilities for each word and class\n",
    "    - alpha: Laplace smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "    - predictions: numpy array, predicted classes\n",
    "    \"\"\"\n",
    "    if issparse(X):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    predictions = []\n",
    "    num_classes, num_features = likelihoods.shape\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        non_zero_indices = X[i].nonzero()[1]\n",
    "\n",
    "        # Calculate the log-likelihood for each class with Laplace smoothing\n",
    "        log_likelihoods = np.log(likelihoods[:, non_zero_indices] + alpha)\n",
    "\n",
    "        # Calculate the log-posterior probability for each class\n",
    "        log_posteriors = np.log(list(priors.values())) + log_likelihoods.sum(axis=1)\n",
    "\n",
    "        # Choose the class with the highest log-posterior probability\n",
    "        predicted_class = np.argmax(log_posteriors)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vocabulary size: 166377\n",
      "Testing vocabulary size: 166377\n",
      "X_train_tfidf type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "X_train_tfidf shape: (160000, 166377)\n",
      "Features in testing but not in training: {163843, 7, 163848, 8, 73746, 165907, 28693, 165909, 163868, 163876, 163877, 22568, 165929, 12334, 163889, 163896, 57403, 47167, 165953, 163907, 51267, 163912, 100425, 155721, 28753, 163924, 165982, 63583, 165990, 163944, 47209, 36980, 32893, 166016, 2179, 166020, 34953, 28816, 45201, 163993, 47259, 164002, 164005, 164010, 166059, 37036, 47274, 164014, 166063, 164016, 8370, 51379, 59572, 61621, 43191, 164027, 164033, 164037, 164038, 164039, 57551, 166098, 164050, 164059, 166109, 162017, 164066, 30946, 121061, 108777, 35051, 164076, 164077, 155884, 166128, 159987, 6391, 28923, 90363, 164093, 164097, 164098, 104707, 164103, 166154, 61707, 164109, 143633, 53523, 53524, 164122, 98587, 22812, 166173, 16670, 102686, 164128, 92442, 39200, 47397, 24872, 160040, 61740, 164141, 121133, 164143, 164145, 147765, 164151, 164152, 47417, 164153, 166208, 14658, 14661, 164165, 162123, 164173, 164180, 61785, 164186, 49498, 164188, 145760, 164193, 164192, 164202, 166252, 164206, 164209, 164216, 33144, 166265, 74109, 164221, 164224, 166284, 398, 121233, 55698, 162195, 141714, 143765, 6556, 166308, 164261, 164264, 166321, 102837, 143801, 164283, 166332, 166340, 166348, 78289, 55765, 12758, 166368, 166369, 49634, 27108, 18916, 147940, 61928, 37374, 14854, 164363, 59918, 164369, 8728, 51746, 164408, 27200, 4673, 164418, 23109, 6732, 57936, 21079, 43620, 162408, 164464, 35443, 39542, 162432, 164482, 164491, 64139, 107149, 162454, 152216, 21148, 164513, 23205, 43689, 164528, 162481, 164533, 29387, 60111, 66259, 142037, 21210, 162525, 164583, 41706, 164588, 164589, 17134, 154349, 64239, 135920, 142065, 164595, 70391, 6911, 164620, 62222, 164624, 152343, 164632, 154402, 11045, 164647, 164648, 31538, 72528, 164689, 62298, 164705, 7010, 164710, 164713, 54126, 7023, 164721, 164723, 164724, 13176, 162700, 45975, 45976, 160691, 164787, 17333, 164791, 62393, 164802, 162756, 164806, 160712, 164813, 54221, 995, 17380, 17383, 54251, 9198, 9201, 160760, 17401, 164859, 164865, 58371, 58373, 17413, 70662, 58376, 17417, 23561, 52236, 17421, 164883, 37911, 162840, 74779, 13341, 160798, 164896, 164900, 101416, 54313, 60456, 164907, 146478, 164921, 19514, 101442, 60493, 58461, 164957, 54365, 58469, 9318, 68714, 146541, 17520, 23675, 9345, 146562, 162945, 126085, 17547, 165007, 91283, 52375, 158873, 160926, 162978, 9379, 165031, 3246, 165053, 165054, 42173, 1218, 38086, 25798, 165068, 165072, 23765, 3288, 21720, 54491, 7390, 21729, 27881, 165098, 21739, 103663, 29938, 68851, 158963, 56574, 83201, 165124, 120072, 58637, 79119, 9489, 40209, 126226, 1299, 70934, 3352, 3366, 9511, 17707, 163121, 154933, 163128, 154937, 70970, 75073, 9567, 163169, 58724, 15719, 58728, 44394, 52587, 19822, 3442, 58768, 9620, 58776, 5531, 130460, 7584, 163232, 165282, 62891, 155053, 103856, 60857, 165311, 157122, 71125, 11743, 17889, 1505, 165349, 73189, 1512, 103917, 73198, 11765, 165384, 163337, 103946, 165389, 7696, 165393, 165403, 58907, 15902, 165407, 1572, 1573, 7720, 157230, 165427, 71225, 165436, 40511, 60995, 165446, 20039, 24149, 11865, 15962, 58970, 58971, 142942, 54893, 58996, 7796, 59000, 120452, 157333, 50839, 128664, 71323, 48801, 151207, 28330, 40619, 40624, 50867, 165557, 71352, 40634, 165562, 165566, 44738, 157389, 59090, 165587, 149206, 36567, 1757, 106214, 108263, 44779, 59118, 104175, 63225, 59129, 165636, 55047, 63246, 61202, 63255, 165657, 157472, 165691, 51004, 59198, 73537, 165707, 165714, 165715, 163670, 1888, 16227, 151403, 24433, 12146, 157555, 163710, 67457, 163715, 67465, 163728, 10130, 165781, 40855, 67479, 163738, 163744, 163746, 163747, 63396, 163752, 165802, 20399, 165807, 10164, 163766, 30648, 28601, 12219, 163774, 53184, 163779, 163780, 163781, 61382, 163782, 100297, 165838, 163796, 165847, 73689, 151514, 163810, 63459, 63463, 165864, 131049, 165865, 165863, 163820, 38893, 165879, 159738, 165886}\n",
      "Class indices in testing: [0 1]\n",
      "Unique classes: 0\n",
      "Unique classes in testing: [0 1]\n",
      "Shape of likelihoods array: (2, 166377)\n",
      "Likelihoods[0] probabilities: [1.23953923e-08 1.23953923e-08 1.24077877e-05 1.23953923e-08\n",
      " 1.23953923e-08 1.23953923e-08 1.23953923e-08 7.16887847e-06\n",
      " 7.16887847e-06 1.23953923e-08]\n",
      "Accuracy: 50.61\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.01      0.02     19979\n",
      "           1       0.50      1.00      0.67     20021\n",
      "\n",
      "    accuracy                           0.51     40000\n",
      "   macro avg       0.72      0.51      0.35     40000\n",
      "weighted avg       0.72      0.51      0.35     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Extract features and target variable\n",
    "X = tweets['processed_tweets']\n",
    "y = tweets['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 5))\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same vocabulary\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Testing vocabulary size: {X_test_tfidf.shape[1]}\")\n",
    "\n",
    "print(f\"X_train_tfidf type: {type(X_train_tfidf)}\")\n",
    "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "training_vocab = set(tfidf_vectorizer.vocabulary_.keys())\n",
    "testing_vocab = set(tfidf_vectorizer.transform(X_test).nonzero()[1])\n",
    "print(f\"Features in testing but not in training: {testing_vocab - training_vocab}\")\n",
    "\n",
    "# Calculate prior and likelihood\n",
    "#priors = calculate_prior(y_train)\n",
    "#likelihoods = calculate_likelihood(X_train_counts, y_train)\n",
    "print(\"Class indices in testing:\", np.unique(y_test))\n",
    "# Calculate prior and likelihood\n",
    "classes, _ = np.unique(y_train)\n",
    "print(\"Unique classes:\", classes)\n",
    "\n",
    "priors = calculate_prior(y_train)\n",
    "print(f\"Unique classes in testing: {np.unique(y_test)}\")\n",
    "likelihoods = calculate_likelihood(X_train_tfidf, y_train, tfidf_vectorizer)\n",
    "\n",
    "print(\"Shape of likelihoods array:\", likelihoods.shape)\n",
    "\n",
    "# Add print statements to debug\n",
    "print(f\"Likelihoods[0] probabilities: {likelihoods[0, :10]}\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = naive_bayes_multinomial(X_test_tfidf, priors, likelihoods)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}\")\n",
    "\n",
    "# Print classification report for more detailed evaluation\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
